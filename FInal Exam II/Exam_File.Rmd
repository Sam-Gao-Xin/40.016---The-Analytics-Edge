---
title: "R Notebook"
output: html_notebook
---



```{r}
library(caTools) #Split train test data
library(ROCR) #ROC Curve
library(rpart) #CART
library(rattle) #CART Visualisation
library(RColorBrewer) #CART Visualisation
library(rpart.plot)  #CART Visualisation
library(ipred) #Bagging for decision trees only
library(randomForest)
library(tm) #Create DTM
library(SnowballC) #Stemming words
library(wordcloud)
library(e1071) #Naive Bayes Classifier
setwd("/Users/james/OneDrive - Singapore University of Technology and Design/SUTD/Year 3/Term 6/40.016 - The Analytics Edge/FInal Exam II")
```


#Qs 1(a)
```{r}
fifa <- read.csv("fifa.csv")
View(fifa)
```

#1(b)
```{r}
players1 <- subset(fifa, Marking>85 & StandingTackle>85 & SlidingTackle>85)
nrow(players1)
table(players1$Position)
#players1 <- fifa$Marking > 85 & fifa$StandingTackle > 85 & fifa$SlidingTackle > 85
```

```{r}
fifa_drop <- subset(fifa, select=-c(ID, Name, Club, Position))
fifa_scale <- scale(fifa_drop)
max(fifa_scale[,35])
min(fifa_scale[,35])
```



```{r}
distances <- dist(fifa_scale, method="euclidean")
```

```{r}
clusterMovies1 <- hclust(distances, method="ward.D2")
plot(clusterMovies1)

clusterGroups1 <- cutree(clusterMovies1, h=20)
length(clusterGroups1)

#length(unique(cutree(clusterGroups1, h = 20)))
```

```{r}
nrow(fifa_scale)
distances <- dist(fifa_scale[,], method="euclidean")

clusterMovies1 <- hclust(distances, method="ward.D2") 
#plot(clusterMovies1)
clusterGroups1 <- cutree(clusterMovies1, k=5)
#table(clusterGroups1$marketing)

Cat1 <- matrix(0,nrow=36,ncol=10) # a matrix "Cat1" where rows denote categories and columns indicate clusters
for(i in 1:36){
  Cat1[i,] <- tapply(fifa_scale[,i], clusterGroups1, mean)
}
rownames(Cat1) <- colnames(fifa_scale)[1:36]
which.max(Cat1["Marking",])

mean(Cat1["Marking",])
mean(Cat1["StandingTackle",])
mean(Cat1["SlidingTackle",])

# clusterMovies2 <- hclust(distances, method="complete") 
# plot(clusterMovies2)
# clusterGroups2 <- cutree(clusterMovies2, k=20)
# table(clusterGroups2)
```

```{r}
df_3 <- subset(fifa, clusterGroups1 == 3)
df_3[, "Position"] 

(mean(df_3$RoomToGrow))
```



```{r}
columns = c("Marking", "StandingTackle", "SlidingTackle")
colMeans(Cat1[columns,])

fifa_drop[Cat1[,3]]

Cat1["Positioning",][3]
```

```{r}
set.seed(123)
clusterMovies2 <- kmeans(fifa_scale,centers=5,nstart=5) 
table(clusterMovies2$cluster)
set.seed(123)
clusterMovies3 <- kmeans(fifa_scale,centers=20, nstart=20) 
table(clusterMovies3$cluster)
```
```{r}
sst <- c()
for (i in 1:15){
  clusters <- kmeans(fifa_scale,centers=i,nstart=20) 
  sst <- c(sst, clusters$tot.withinss)
}
plot(1:15,sst) 
```
# QS 2
```{r}
df2 <- read.csv("dendrohydrology.csv")
head(df2)
dim(df2)
train <- df2[1:80,]
test <- df2[81:91,]
sort((cor(train)[,"Q"]))
```

```{r}
library(rpart)
set.seed(100)
cart1 <- rpart(Q~.,data=train,method="anova")


predictcart1 <- predict(cart1,newdata=test,type="matrix")
mean((predictcart1-test$Q)^2)
#MSE(test$Q, predictcart1)

prp(cart1)
printcp(cart1)
```
```{r}
cart2 <- prune(cart1,0.085482)
predictcart2 <- predict(cart2,newdata=test,type="matrix")
mean((predictcart2-test$Q)^2)
prp(cart2)
```

```{r}
set.seed(123)
mt <- bagging (Q~.,data=train, coob=TRUE)
print(mt)
predict_bag_model <- predict(mt,newdata=test,type="class")
mean((test$Q - predict_bag_model)^2)
plot(predict_bag_model)
plot(predictcart2)
```

```{r}
head(df2)
dim(df2)
df2$Q
```

```{r}
meanQ <- mean(df2$Q)
meanQ
df2$category <- ifelse(df2$Q>meanQ, "WET", "DRY")
table(df2$category)

df2 <- subset(df2, select=-c(Q))
train1 <- df2[1:80,]
test1 <-df2[81:91,]

dim(train1)
dim(test1)

table(train1$category)

table(test1$category)
```
```{r}
set.seed(123)
forest_model <- randomForest(as.factor(category)~., data=train1)
print(forest_model)
predictforest <- predict(forest_model,newdata=test1,type="class")

sum(diag(table(test1$category,predictforest)))/sum(table(test1$category,predictforest))
```

```{r}
varImpPlot(forest_model)
order2 <- sort(varUsed(forest_model), decreasing=TRUE, index.return=TRUE) #After running predict(), we can see what was actually the most useful variables
names(test1[,head(order2$ix)]) #Print out the names of the variables that are the most useful
```
```{r}
set.seed(123)
model3 <- naiveBayes(as.factor(category)~.,data=train1)

predict3 <- predict(model3,newdata=test1,type="class")

sum(diag(table(test1$category,predict3)))/sum(table(test1$category,predict3))
```

```{r}
model3
```
```{r}
cor(df2)
```

