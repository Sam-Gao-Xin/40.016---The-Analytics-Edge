rm(list=ls()) # Clear the environment
setwd("/Users/james/OneDrive - Singapore University of Technology and Design/SUTD/Year 3/Term 6/40.016 - The Analytics Edge/Week 9/Lecture 14 - Bagging and Random Forest")  # Setup the working directory
# Load data
supreme <- read.csv("supreme(8).csv") # We have 623 observations and 20 variables
str(supreme)      # Internal structure of the dataframe
# Prepare the output variable (justice Stevens' decision)
stevens <- subset(supreme[,c("docket","term","stevdir","petit","respon","circuit","unconst","lctdir","issue","result")],supreme$stevdir!=9)
stevens$rev <- as.integer((stevens$lctdir=="conser" & stevens$stevdir==0) | (stevens$lctdir=="liberal" & stevens$stevdir==1))
# Prepare the output variable (justice Stevens' decision)
stevens <- subset(supreme[,c("docket","term","stevdir","petit","respon","circuit","unconst","lctdir","issue","result")],supreme$stevdir!=9)
stevens$rev <- as.integer((stevens$lctdir=="conser" & stevens$stevdir==0) | (stevens$lctdir=="liberal" & stevens$stevdir==1))
if(!require(caTools)){                          # install or load the package
install.packages("caTools")
library(caTools)
}
set.seed(123)                                   # set seed for random sampling
spl <- sample.split(stevens$rev,SplitRatio=0.7) # We use 70% of the data for training
train <- subset(stevens,spl==TRUE);             # training dataset
test <- subset(stevens,spl==FALSE);             # testing dataset
test <- subset(test,test$issue!="IR")           # remove the only realization of the IR value (please refer to Week 8's material)
library(rpart)
cart1 <- rpart(rev~petit+respon+circuit+lctdir+issue+unconst,data=train,method="class") # build the tree
predictcart1 <- predict(cart1,newdata=test,type="class") # prediction on the test dataset
table(test$rev,predictcart1) # confusion matrix
opt <- which.min(cart1$cptable[,"xerror"]) # get index of CP with lowest xerror
cp <- cart1$cptable[opt, "CP"]             # get the corresponding value
cart2 <- prune(cart1,cp)                   # prune
predictcart2 <- predict(cart2,newdata=test,type="class") # prediction on the test dataset on the pruned tree
table(test$rev,predictcart2)               # confusion matrix
if(!require(ipred)){
install.packages("ipred")
library(ipred)
}
mt <- bagging (as.factor(rev)~petit+respon+circuit+lctdir+issue+unconst,data=train, coob=TRUE)
print(mt)     # short summary
predict_bag_model <- predict(mt,newdata=test,type="class")
table(test$rev,predict_bag_model)
if(!require(randomForest)){
install.packages("randomForest")
library(randomForest)
}
forest <- randomForest(as.factor(rev)~petit+respon+circuit+unconst+lctdir+issue,data=train)
forest
predictforest <- predict(forest,newdata=test,type="class")
table(test$rev,predictforest)
importance(forest) # tabulated results
varImpPlot(forest) # plot
importance(forest) # tabulated results
varUsed(forest, by.tree=FALSE, count=TRUE)
# Values of ntree to be tested
B <- seq(5,500,by=5)
# Values of ntree to be tested
B <- seq(5,500,by=5)
# Initialize a vector for the OOB value
OOB <- vector(); OOB <- c(OOB, 1:length(B))
# For loop
for (i in 1:length(B)){
# train a forest with B[i] trees
forest_temp <- randomForest(as.factor(rev)~petit+respon+circuit+unconst+lctdir+issue,data=train,ntree=B[i])
# model performance
OOB[i] <- forest_temp$err.rate[B[i],1]
# remove the temporary variable
rm(forest_temp)
}
library(ggplot2)
ggplot(data = data.frame(ntree=B, error=OOB),
mapping = aes(x = ntree, y = error)) +
geom_point(size=3) +
coord_cartesian(xlim=c(0,500), ylim=c(0.325,0.450)) +
labs(x="Number of Trees", y="OOB error")
# Values of ntree to be tested
B <- seq(5,500,by=5)
# Values of mtry to be tested
m <- seq(1,6,by=1)
# Initialize a matrix for the OOB value
OOB_matrix <- matrix(0,nrow=length(B),ncol=length(m))
# For loop
for (i in 1:length(B)){
for (j in 1:length(m)){
# train a forest with B[i] trees
forest_temp <- randomForest(as.factor(rev)~petit+respon+circuit+unconst+lctdir+issue,data=train,ntree=B[i],mtry=m[j])
# model performance
OOB_matrix[i,j] <- forest_temp$err.rate[B[i],1]
# remove the temporary variable
rm(forest_temp)
}
}
library(reshape2)
rownames(OOB_matrix) <- B
colnames(OOB_matrix) <- m
# longData <- as.data.frame(OOB_matrix)
# row.names(longData) <- B; colnames(longData) <- m
longData <- melt(OOB_matrix)
# ...
ggplot(longData, aes(x = Var1, y = Var2)) +
geom_raster(aes(fill=value)) +
scale_fill_gradient(low="grey90", high="red") +
labs(x="Number of Trees", y="Number of predictors")
which.min(OOB_matrix[,2])
best_forest <- randomForest(as.factor(rev)~petit+respon+circuit+unconst+lctdir+issue,data=train,mtry=2,ntree=300)
# best_forest
predict_bestforest <- predict(best_forest,newdata=test,type="class")
table(test$rev,predict_bestforest)
library(caret)
control <- trainControl(method="repeatedcv", number=10, repeats=1, search="grid")
# set.seed(seed)
tunegrid <- expand.grid(.mtry=c(1:6))
rf_gridsearch <- train(as.factor(rev)~petit+respon+circuit+unconst+lctdir+issue, data=train, method="rf", metric="Accuracy", tuneGrid=tunegrid, trControl=control)
