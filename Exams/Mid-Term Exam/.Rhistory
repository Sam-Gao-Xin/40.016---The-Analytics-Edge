#1(n)
df2 <- subset(df1, select=-(States+Crime))
str(df2)
#1(n)
df2 <- subset(df1, select=-(StatesCrime))
#1(n)
df2 <- subset(df1, select=-(States))
str(df2)
#1(n)
df2 <- subset(df1, select=-(States & Crime))
str(df2)
#1(n)
df2 <- subset(df1, select=-(States,Crime))
#1(n)
df2 <- subset(df1, select=-(States+Crime))
str(df2)
#1(n)
df2 <- subset(df1, select=-c(States+Crime))
str(df2)
#1(n)
df2 <- subset(df1, select=-c(States,Crime))
str(df2)
#1(o)
pca_output <- prcomp(df2,scale=T)
summary(pca_output)
pve<-pca_output$sdev^2/sum(pca_output$sdev^2)
cpve<-cumsum(pve)
cpve
#1(n)
df2 <- df1
rownames(df2) <- df2
rownames(df2) <- df2$States
df2 <- subset(df2, select=-c(States,Crime))
str(df2)
pca_output <- prcomp(df,scale=T)
summary(pca_output)
head(df2)
#1(n)
df2 <- df1
rownames(df2) <- df2$States
df2 <- subset(df2, select=-c(States,Crime))
str(df2)
pca_output <- prcomp(df,scale=T)
summary(pca_output)
#1(o)
pca_output <- prcomp(df2,scale=T)
summary(pca_output)
fviz_pca_biplot(pca_output, label = "var", habillage=dependent_var_name, addEllipses=TRUE, ellipse.level=0.95)
fviz_pca_biplot(pca_output, label = "var", addEllipses=TRUE, ellipse.level=0.95)
fviz_eig(pca_output)
fviz_pca_biplot(pca_output, label = "var", habillage=dependent_var_name, addEllipses=TRUE, ellipse.level=0.95)
fviz_pca_biplot(pca_output, label = "var", addEllipses=TRUE, ellipse.level=0.95)
#1(o)
pca_output <- prcomp(df2,scale=T)
summary(pca_output)
fviz_eig(pca_output)
pca_output$sdev
setwd("/Users/james/OneDrive - Singapore University of Technology and Design/SUTD/Year 3/Term 6/40.016 - The Analytics Edge/Exercise/Week 2")
auto<-read.csv("Auto(3).csv")
str(auto)
auto$horsepower <- as.numeric(as.character(auto$horsepower))
model1<- lm(mpg~horsepower, data=auto)
summary(model1)
predict.lm(model1,newdata=data.frame(horsepower=98),interval=c("confidence"),level=.99)
cor(auto$mpg,auto$horsepower, use = "pairwise.complete.obs")
cor(auto$mpg,auto$horsepower, use = "pairwise.complete.obs")^2
library(ggplot2)
ggplot(auto,aes(horsepower,mpg))+geom_point(na.rm=T)+geom_smooth(method="lm",na.rm=T,se=F)
# Alternative plotting
#plot(auto$horsepower,auto$mpg)
#abline(model1)
library(ggfortify)
autoplot(model1,label.size = 3)
# Alternative plotting without ggplot
#layout(matrix(1:4,2,2))
#plot(model1)
library(psych)
pairs.panels(auto,ellipses = F, lm =T, breaks=10, hist.col="blue")
str(auto)
auto1<-subset(auto,select=-c(name))
str(auto1)
cor(auto1, use = "pairwise.complete.obs")
model2 <-lm(mpg~., data=auto1)
summary(model2)
set.seed(1)
x1 <- runif(100)
x2 <- 0.5*x1 + rnorm(100)/10
y <- 2 + 2*x1 + 0.3*x2 + rnorm(100)
cor(x1,x2)
library(ggplot2)
ggplot(cbind(x1,x2,y),aes(x1,x2))+geom_point()
model3 <- lm(y~x1+x2)
summary(model3)
model4 <- lm(y~x1)
summary(model4)
model5 <- lm(y~x2)
summary(model5)
boston <- read.csv("Boston(4).csv")
colnames(boston)
model1 <- lm(medv~crim, data=boston)
model2 <- lm(medv~zn, data=boston)
model3 <- lm(medv~indus, data=boston)
model4 <- lm(medv~chas, data=boston)
model5 <- lm(medv~nox, data=boston)
model6 <- lm(medv~rm, data=boston)
model7 <- lm(medv~age, data=boston)
model8 <- lm(medv~dis, data=boston)
model9 <- lm(medv~rad, data=boston)
model10 <- lm(medv~tax, data=boston)
model11 <- lm(medv~ptratio, data=boston)
model12 <- lm(medv~black, data=boston)
model13 <- lm(medv~lstat, data=boston)
summary(model1)
# Verify this for all the models by checking that the p-values are close to 0.
summary(model13)
ggplot(boston,aes(lstat,medv))+geom_point(na.rm=T)+geom_smooth(method="lm",na.rm=T,se=F)
modelall<- lm(medv~., data=boston)
summary(modelall)
Ind <- c(model1$coef[2], model2$coef[2], model3$coef[2], model4$coef[2], model5$coef[2],
model6$coef[2], model7$coef[2], model8$coef[2], model9$coef[2], model10$coef[2],
model11$coef[2], model12$coef[2], model13$coef[2])
Ind
All <- modelall$coef[2:14]
All
ggplot(cbind(Ind,All),aes(Ind,All)) + geom_point()+geom_smooth(method="lm",se=F)+ggtitle("Coefficient relationship") + xlab("Simple linear regression") + ylab("Multiple linear regression")
summary(model13)
modelpoly2 <- lm(medv~poly(lstat,2,raw=TRUE), data = boston)
summary(modelpoly2)
modelpoly3 <- lm(medv~poly(lstat,3,raw=TRUE), data = boston)
summary(modelpoly3)
modelpoly4 <- lm(medv~poly(lstat,4,raw=TRUE), data = boston)
summary(modelpoly4)
modelpoly5 <- lm(medv~poly(lstat,5,raw=TRUE), data = boston)
summary(modelpoly5)
modelpoly6 <- lm(medv~poly(lstat,6,raw=TRUE), data = boston)
summary(modelpoly6)
boston$pr1 <- predict(model13,newdata=boston)
boston$pr5 <- predict(modelpoly5,newdata=boston)
ggplot(boston)+geom_point(aes(lstat,medv))+geom_line(aes(lstat,pr1),color="blue",size=2)+geom_line(aes(lstat,pr5),color="red",linetype="solid",size=2)
wine<-read.csv("winedata(3).csv")
str(wine)
wine$age91<-1991-wine$vintage
wine$age92<-1992-wine$vintage
mean(subset(wine$price91,wine$age91>=15))
mean(subset(wine$price91,wine$hrain<mean(wine$hrain)&wine$tempdiff<mean(wine$tempdiff)))
train<-subset(wine,vintage<=1981)
model1<-lm(log(price91)~age91,data=train)
summary(model1)
confint(model1, level = 0.99)
test<-subset(wine,vintage>=1982)
predtest<-predict(model1,newdata=test)
sse<-sum((log(test$price91)-predtest)^2)
sst<-sum((log(test$price91)-mean(log(train$price91)))^2)
testR2<- 1-sse/sst
testR2
model2<-lm(log(price91)~temp+hrain+wrain+tempdiff+age91,data=train)
summary(model2)
model2a<-lm(log(price91)~age91,data=train)
summary(model2a)
model3<-lm(log(price91)~temp+hrain+age91,data=train)
summary(model3)
model4<-lm(log(price92)~temp+hrain+age92,data=train)
summary(model4)
nrow(iris)
ncol(iris)
dim(iris)
colnames(iris)
head(iris)
iris.data<-iris[,-5]
iris.sp<-iris[,5]
library(psych)
pairs.panels(iris.data, ellipses = F, lm =T, breaks=10, hist.col="blue")
pr.out<-prcomp(iris.data,scale=F)
summary(pr.out)
pr.out$sdev
pve<-pr.out$sdev^2/sum(pr.out$sdev^2)
cpve<-cumsum(pve)
pve
cpve
plot(cpve,xlab="Principal components",type="l",ylim=c(0.7,1))
pr.out$rotation
summary(pca_output)
pca_output$rotation
pca_output[,1$rotation
pca_output[,1]$rotation
pca_output[,1]$rotation
pca_output$rotation
pca_output[1]$rotation
pca_output[2]$rotation
pca_output$rotation
pca_output$x
pca_output$x[PC1]
pca_output$x[2]
which.max(pca_output$x)
df1[df1$state == "MS"]
head(df1[df1$state == "MS"])
df1[df1$state == MS]
df1[df1$state == "MS"]
View(df1)
#Qs2
#2(a)
transport <- read.csv("transport(1).csv")
str(transport)
table(transport$MODE)
sapply(transport$MODE, sum)
table(transport$MODE)
plot(transport$MODE)
table(transport$MODE)
table(transport$MODE)
transport[transport$TTME == 0]
View(transport)
table(transport)
table(transport$MODE)
str(transport)
tapply(transport$CHOICE, transport$MODE, FUN=sum)
#2(d)
set.seed(200)
spl <- sample(210, 0.7*210)
training <- subset(transport, is.element(ID, spl))
tapply(training$CHOICE, training$MODE, FUN=sum)
48/(41+48+15+43)
spl
transport-spl
!spl
head(transport)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "CHOICE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = c(2), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "CHOICE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = c(2), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
head(heating)
setwd("/Users/james/OneDrive - Singapore University of Technology and Design/SUTD/Year 3/Term 6/40.016 - The Analytics Edge/Exercise/Week 4")
rm(list=ls())
#install.packages(mlogit)
library(mlogit)
heating <- read.csv("Heating(7).csv")
head(heating)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = c(3:7), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
rm(list=ls())
library(ggplot2) #Standard Data Visualisation Plotting library
library(ggfortify) #Aids Plotting linear plots with ggplot2
library(psych) #Create a scatter plot matrix
library(factoextra) #Visualisation of the PCA eigen values
library(caTools) #Perform train-test split on dataframe
suppressMessages(library(ROCR)) #AUC-ROC Package
library(mlogit) #Multinomial Logistics Regression
library(leaps) #Subset selection
#Qs2
#2(a)
transport <- read.csv("transport(1).csv")
str(transport)
head(transport)
tapply(transport$CHOICE, transport$MODE, FUN=sum)
#2(c)
set.seed(200)
spl <- sample(210, 0.7*210)
training <- subset(transport, is.element(ID, spl))
#2(d)
tapply(training$CHOICE, training$MODE, FUN=sum)
48/(41+48+15+43)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = c(3:7), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
head(transport)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = c(4:7), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
setwd("/Users/james/OneDrive - Singapore University of Technology and Design/SUTD/Year 3/Term 6/40.016 - The Analytics Edge/Exercise/Week 5")
suppressMessages(library(mlogit))  # suppress dependency loads
df_1_a <- read.csv("Heating(7).csv")
str(df_1_a)
# note that we get different column order from dataset
# c("ec", "er", ...) vs c("gc", "gr", ...)
choices_1_a <- get_choices(df_1_a, "depvar")
pred_cols_1 <- get_all_pred_cols(df_1_a, choices_1_a)
vary_ind_1 <- which(names(df_1_a) %in% pred_cols_1)
if (is(input_dataframe[,choice_colname], "factor")) {
choices <- levels(input_dataframe[,choice_colname])
} else {
choices <- unique(input_dataframe[,choice_colname])
}
return(choices)
get_choices <- function(input_dataframe, choice_colname) {
if (is(input_dataframe[,choice_colname], "factor")) {
choices <- levels(input_dataframe[,choice_colname])
} else {
choices <- unique(input_dataframe[,choice_colname])
}
return(choices)
}
get_all_pred_cols <- function(input_dataframe, choices) {
pred_cols <- c()
for (colname in colnames(input_dataframe)) {
if (any(endsWith(colname, as.character(choices)))) {
pred_cols <- c(pred_cols, colname)
}
}
return(pred_cols)
}
get_pred_vars <- function(input_pred_cols, choices) {
single_choice_pred <- input_pred_cols[seq(1,
length(input_pred_cols),
length(choices))]
for (choice in choices) {
if (all(endsWith(single_choice_pred, as.character(choice)))) {
pred_vars_choice_rem <- gsub(paste0(".{", nchar(choice), "}$"), '',
single_choice_pred)
if (any(endsWith(pred_vars_choice_rem, "."))) {
return(gsub(".{1}$", '', pred_vars_choice_rem))
} else {
return(pred_vars_choice_rem)
}
}
}
}
We can then just use the functions as defined:
```{r 1_a3}
# note that we get different column order from dataset
# c("ec", "er", ...) vs c("gc", "gr", ...)
choices_1_a <- get_choices(df_1_a, "depvar")
pred_cols_1 <- get_all_pred_cols(df_1_a, choices_1_a)
vary_ind_1 <- which(names(df_1_a) %in% pred_cols_1)
pred_vars_1 <- get_pred_vars(pred_cols_1, choices_1_a)
# running mlogit.data on what we have to transform it
data_1_a <- mlogit.data(df_1_a,  # data.frame of data
choice = "depvar",  # column name of choice
shape = "wide",  # wide means each row is an observation
# long if each row is an alternative
varying = vary_ind_1,
# indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
pred_cols_1 <- get_all_pred_cols(df_1_a, choices_1_a)
vary_ind_1 <- which(names(df_1_a) %in% pred_cols_1)
vary_ind_1
#Qs2
#2(a)
transport <- read.csv("transport(1).csv")
str(transport)
head(transport)
tapply(transport$CHOICE, transport$MODE, FUN=sum)
#2(c)
set.seed(200)
spl <- sample(210, 0.7*210)
training <- subset(transport, is.element(ID, spl))
#2(d)
tapply(training$CHOICE, training$MODE, FUN=sum)
48/(41+48+15+43)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = c(4:7), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
get_choices <- function(input_dataframe, choice_colname) {
if (is(input_dataframe[,choice_colname], "factor")) {
choices <- levels(input_dataframe[,choice_colname])
} else {
choices <- unique(input_dataframe[,choice_colname])
}
return(choices)
}
get_all_pred_cols <- function(input_dataframe, choices) {
pred_cols <- c()
for (colname in colnames(input_dataframe)) {
if (any(endsWith(colname, as.character(choices)))) {
pred_cols <- c(pred_cols, colname)
}
}
return(pred_cols)
}
get_pred_vars <- function(input_pred_cols, choices) {
single_choice_pred <- input_pred_cols[seq(1,
length(input_pred_cols),
length(choices))]
for (choice in choices) {
if (all(endsWith(single_choice_pred, as.character(choice)))) {
pred_vars_choice_rem <- gsub(paste0(".{", nchar(choice), "}$"), '',
single_choice_pred)
if (any(endsWith(pred_vars_choice_rem, "."))) {
return(gsub(".{1}$", '', pred_vars_choice_rem))
} else {
return(pred_vars_choice_rem)
}
}
}
}
choices_1_a <- get_choices(training, "MODE")
pred_cols_1 <- get_all_pred_cols(training, choices_1_a)
vary_ind_1 <- which(names(training) %in% pred_cols_1)
pred_vars_1 <- get_pred_vars(pred_cols_1, choices_1_a)
vary_ind_1
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = c(4:7), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = c(4:9), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
str(transport)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = c(3:9), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = c(3:9), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
S<- dfidx(training, shape="long", choice="CHOICE", varying =c(3:9), sep="")
S<- dfidx(training, shape="long", choice="CHOICE", varying =c(3:9))
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = c(3:9), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
sep = "."  # not necessary but still good to be clear
)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
sep = "."  # not necessary but still good to be clear
)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = c(-3), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = -c(3), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = -c(1:3), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
#2(e)
data_transport <- mlogit.data(training,  # data.frame of data
choice = "MODE",  # column name of choice
shape = "long",  # wide means each row is an observation
# long if each row is an alternative
varying = -c(1,3), # indices of varying columns for each alternative,
sep = "."  # not necessary but still good to be clear
)
str(transport)
